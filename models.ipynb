{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-29T12:33:06.998889Z",
     "iopub.status.busy": "2026-01-29T12:33:06.998257Z",
     "iopub.status.idle": "2026-01-29T12:33:07.286947Z",
     "shell.execute_reply": "2026-01-29T12:33:07.286163Z",
     "shell.execute_reply.started": "2026-01-29T12:33:06.998860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cybersecurity-dataset/CIC.csv\n",
      "/kaggle/input/cybersecurity-dataset/unswnb15.csv\n",
      "/kaggle/input/cybersecurity-dataset/IOT-23.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T16:10:06.317680Z",
     "iopub.status.busy": "2026-01-29T16:10:06.317005Z",
     "iopub.status.idle": "2026-01-29T16:42:56.022434Z",
     "shell.execute_reply": "2026-01-29T16:42:56.021774Z",
     "shell.execute_reply.started": "2026-01-29T16:10:06.317651Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== IoT23_binary =====\n",
      "Class distribution: {0: 1923, 1: 21222}\n",
      "IoT23_binary/flow.x: shape=(23145, 6) finite_frac=1.000000 min=0.0000 max=18.1403\n",
      "[IoT23_binary HetGNN] ep=001 loss=0.6826 val_acc=0.9899 val_macroF1=0.9652\n",
      "[IoT23_binary HetGNN] ep=020 loss=0.0163 val_acc=0.9945 val_macroF1=0.9825\n",
      "[IoT23_binary HetGNN] ep=040 loss=0.0015 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HetGNN] ep=060 loss=0.0008 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HetGNN] ep=080 loss=0.0008 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HetGNN] ep=100 loss=0.0008 val_acc=1.0000 val_macroF1=1.0000\n",
      "IoT23_binary HetGNN test_acc=0.9994 test_macroF1=0.9981\n",
      "[IoT23_binary R-GCN] ep=001 loss=1.4278 val_acc=0.9170 val_macroF1=0.4784\n",
      "[IoT23_binary R-GCN] ep=020 loss=0.0342 val_acc=0.9977 val_macroF1=0.9925\n",
      "[IoT23_binary R-GCN] ep=040 loss=0.0072 val_acc=0.9997 val_macroF1=0.9991\n",
      "[IoT23_binary R-GCN] ep=060 loss=0.0031 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary R-GCN] ep=080 loss=0.0011 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary R-GCN] ep=100 loss=0.0010 val_acc=1.0000 val_macroF1=1.0000\n",
      "IoT23_binary R-GCN test_acc=0.9991 test_macroF1=0.9972\n",
      "[IoT23_binary HGT] ep=001 loss=0.6947 val_acc=0.0830 val_macroF1=0.0766\n",
      "[IoT23_binary HGT] ep=020 loss=0.1010 val_acc=0.9960 val_macroF1=0.9867\n",
      "[IoT23_binary HGT] ep=040 loss=0.0111 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HGT] ep=060 loss=0.0014 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HGT] ep=080 loss=0.0018 val_acc=1.0000 val_macroF1=1.0000\n",
      "[IoT23_binary HGT] ep=100 loss=0.0008 val_acc=1.0000 val_macroF1=1.0000\n",
      "IoT23_binary HGT test_acc=0.9997 test_macroF1=0.9991\n",
      "\n",
      "===== CIC_label =====\n",
      "Class distribution: {0: 50000, 1: 50000}\n",
      "CIC_label/flow.x: shape=(100000, 80) finite_frac=1.000000 min=-26.0824 max=21.8030\n",
      "[CIC_label HetGNN] ep=001 loss=0.6882 val_acc=0.9605 val_macroF1=0.9604\n",
      "[CIC_label HetGNN] ep=020 loss=0.0010 val_acc=0.9997 val_macroF1=0.9997\n",
      "[CIC_label HetGNN] ep=040 loss=0.0004 val_acc=0.9998 val_macroF1=0.9998\n",
      "[CIC_label HetGNN] ep=060 loss=0.0004 val_acc=0.9998 val_macroF1=0.9998\n",
      "[CIC_label HetGNN] ep=080 loss=0.0003 val_acc=0.9998 val_macroF1=0.9998\n",
      "[CIC_label HetGNN] ep=100 loss=0.0003 val_acc=0.9998 val_macroF1=0.9998\n",
      "CIC_label HetGNN test_acc=0.9997 test_macroF1=0.9997\n",
      "[CIC_label R-GCN] ep=001 loss=0.8264 val_acc=0.5081 val_macroF1=0.3510\n",
      "[CIC_label R-GCN] ep=020 loss=0.0039 val_acc=0.9993 val_macroF1=0.9993\n",
      "[CIC_label R-GCN] ep=040 loss=0.0026 val_acc=0.9997 val_macroF1=0.9997\n",
      "[CIC_label R-GCN] ep=060 loss=0.0017 val_acc=0.9997 val_macroF1=0.9997\n",
      "[CIC_label R-GCN] ep=080 loss=0.0010 val_acc=0.9999 val_macroF1=0.9999\n",
      "[CIC_label R-GCN] ep=100 loss=0.0009 val_acc=0.9999 val_macroF1=0.9999\n",
      "CIC_label R-GCN test_acc=0.9997 test_macroF1=0.9997\n",
      "[CIC_label HGT] ep=001 loss=0.6922 val_acc=0.4999 val_macroF1=0.3337\n",
      "[CIC_label HGT] ep=020 loss=0.0039 val_acc=0.9998 val_macroF1=0.9998\n",
      "[CIC_label HGT] ep=040 loss=0.0006 val_acc=0.9999 val_macroF1=0.9999\n",
      "[CIC_label HGT] ep=060 loss=0.0004 val_acc=0.9999 val_macroF1=0.9999\n",
      "[CIC_label HGT] ep=080 loss=0.0004 val_acc=0.9999 val_macroF1=0.9999\n",
      "[CIC_label HGT] ep=100 loss=0.0002 val_acc=0.9999 val_macroF1=0.9999\n",
      "CIC_label HGT test_acc=0.9997 test_macroF1=0.9997\n",
      "\n",
      "===== UNSW_attack_cat =====\n",
      "Class distribution: {0: 2677, 1: 2329, 2: 16353, 3: 44525, 4: 24246, 5: 58871, 6: 93000, 7: 13987, 8: 1511}\n",
      "UNSW_attack_cat/flow.x: shape=(257499, 11) finite_frac=1.000000 min=0.0000 max=22.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNSW_attack_cat HetGNN] ep=001 loss=2.2037 val_acc=0.0879 val_macroF1=0.0681\n",
      "[UNSW_attack_cat HetGNN] ep=020 loss=1.5455 val_acc=0.5074 val_macroF1=0.2737\n",
      "[UNSW_attack_cat HetGNN] ep=040 loss=1.3391 val_acc=0.5724 val_macroF1=0.3219\n",
      "[UNSW_attack_cat HetGNN] ep=060 loss=1.2671 val_acc=0.6002 val_macroF1=0.3526\n",
      "[UNSW_attack_cat HetGNN] ep=080 loss=1.2289 val_acc=0.6149 val_macroF1=0.3816\n",
      "[UNSW_attack_cat HetGNN] ep=100 loss=1.1981 val_acc=0.6207 val_macroF1=0.3886\n",
      "UNSW_attack_cat HetGNN test_acc=0.6209 test_macroF1=0.3896\n",
      "[UNSW_attack_cat R-GCN] ep=001 loss=3.2023 val_acc=0.1672 val_macroF1=0.1073\n",
      "[UNSW_attack_cat R-GCN] ep=020 loss=1.5800 val_acc=0.5263 val_macroF1=0.3021\n",
      "[UNSW_attack_cat R-GCN] ep=040 loss=1.4488 val_acc=0.5744 val_macroF1=0.3176\n",
      "[UNSW_attack_cat R-GCN] ep=060 loss=1.3857 val_acc=0.5959 val_macroF1=0.3264\n",
      "[UNSW_attack_cat R-GCN] ep=080 loss=1.3538 val_acc=0.5846 val_macroF1=0.3269\n",
      "[UNSW_attack_cat R-GCN] ep=100 loss=1.3302 val_acc=0.6028 val_macroF1=0.3338\n",
      "UNSW_attack_cat R-GCN test_acc=0.5860 test_macroF1=0.3465\n",
      "[UNSW_attack_cat HGT] ep=001 loss=2.1997 val_acc=0.2455 val_macroF1=0.0839\n",
      "[UNSW_attack_cat HGT] ep=020 loss=1.8071 val_acc=0.5684 val_macroF1=0.2382\n",
      "[UNSW_attack_cat HGT] ep=040 loss=1.4896 val_acc=0.5533 val_macroF1=0.2996\n",
      "[UNSW_attack_cat HGT] ep=060 loss=1.3269 val_acc=0.5807 val_macroF1=0.3196\n",
      "[UNSW_attack_cat HGT] ep=080 loss=1.2470 val_acc=0.5948 val_macroF1=0.3340\n",
      "[UNSW_attack_cat HGT] ep=100 loss=1.2144 val_acc=0.6147 val_macroF1=0.3689\n",
      "UNSW_attack_cat HGT test_acc=0.6218 test_macroF1=0.3849\n",
      "\n",
      "Done. Check outputs/\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import Linear, RGCNConv\n",
    "\n",
    "# HGT optional\n",
    "try:\n",
    "    from torch_geometric.nn import HGTConv  # noqa\n",
    "    _HGT_AVAILABLE = True\n",
    "except Exception:\n",
    "    _HGT_AVAILABLE = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_text(path: str, text: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def signed_log1p_np(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "\n",
    "def sanitize_numeric_frame(\n",
    "    df: pd.DataFrame,\n",
    "    cols: Optional[List[str]] = None,\n",
    "    *,\n",
    "    transform: str = \"none\",  # \"none\" | \"log1p\" | \"signed_log1p\"\n",
    "    clip_lower: Optional[float] = None,  # e.g. 0.0 if you know features are non-negative\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a numeric-only DataFrame with robust handling of NaN/inf.\n",
    "    \"\"\"\n",
    "    if cols is None:\n",
    "        X = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    else:\n",
    "        X = df[cols].copy()\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    if clip_lower is not None:\n",
    "        X = X.clip(lower=clip_lower)\n",
    "\n",
    "    arr = X.values.astype(np.float64, copy=False)\n",
    "\n",
    "    if transform == \"log1p\":\n",
    "        # log1p requires x > -1; if you expect non-negative features, set clip_lower=0.0\n",
    "        arr = np.log1p(arr)\n",
    "    elif transform == \"signed_log1p\":\n",
    "        arr = signed_log1p_np(arr)\n",
    "    elif transform != \"none\":\n",
    "        raise ValueError(\"transform must be one of: none, log1p, signed_log1p\")\n",
    "\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return pd.DataFrame(arr, columns=X.columns)\n",
    "\n",
    "\n",
    "def debug_tensor_stats(name: str, x: torch.Tensor) -> str:\n",
    "    x_cpu = x.detach().cpu()\n",
    "    finite = torch.isfinite(x_cpu)\n",
    "    frac = float(finite.float().mean().item())\n",
    "    mn = float(torch.nan_to_num(x_cpu, nan=0.0, posinf=0.0, neginf=0.0).min().item())\n",
    "    mx = float(torch.nan_to_num(x_cpu, nan=0.0, posinf=0.0, neginf=0.0).max().item())\n",
    "    return f\"{name}: shape={tuple(x_cpu.shape)} finite_frac={frac:.6f} min={mn:.4f} max={mx:.4f}\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Confusion matrix figures\n",
    "# =============================\n",
    "def save_confusion_matrix(\n",
    "    cm: np.ndarray,\n",
    "    labels: List[str],\n",
    "    out_base: str,\n",
    "    title: str,\n",
    "    normalize: Optional[str] = None,  # None | \"true\" (row) | \"pred\" (col)\n",
    "    dpi: int = 300,\n",
    "):\n",
    "    cm = np.asarray(cm, dtype=float)\n",
    "\n",
    "    if normalize is None:\n",
    "        disp = cm\n",
    "        annot = cm.astype(int).astype(str)\n",
    "    else:\n",
    "        if normalize == \"true\":\n",
    "            denom = cm.sum(axis=1, keepdims=True)\n",
    "        elif normalize == \"pred\":\n",
    "            denom = cm.sum(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"normalize must be None, 'true', or 'pred'\")\n",
    "        denom[denom == 0] = 1.0\n",
    "        disp = cm / denom\n",
    "        annot = np.vectorize(lambda x: f\"{x:.2f}\")(disp)\n",
    "\n",
    "    n = len(labels)\n",
    "    figsize = (max(6.5, 0.38 * n), max(5.5, 0.38 * n))\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    if normalize is None:\n",
    "        im = ax.imshow(disp, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "        vmax = disp.max()\n",
    "        thresh = vmax * 0.6\n",
    "    else:\n",
    "        im = ax.imshow(disp, interpolation=\"nearest\",\n",
    "                       cmap=\"Blues\", vmin=0, vmax=1)\n",
    "        thresh = 0.6\n",
    "\n",
    "\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax.set_ylabel(\"True\", fontsize=12)\n",
    "\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_yticks(np.arange(n))\n",
    "    ax.set_xticklabels(labels, fontsize=10, rotation=90)\n",
    "    ax.set_yticklabels(labels, fontsize=10)\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            val = disp[i, j]\n",
    "            ax.text(\n",
    "                j, i, annot[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=9,\n",
    "                color=(\"white\" if val > thresh else \"black\")\n",
    "            )\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(out_base + \".png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "    fig.savefig(out_base + \".pdf\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Baselines\n",
    "# =============================\n",
    "@dataclass\n",
    "class BaselineResult:\n",
    "    name: str\n",
    "    report: str\n",
    "    cm: np.ndarray\n",
    "\n",
    "\n",
    "def run_baselines(\n",
    "    X: pd.DataFrame,\n",
    "    y_enc: pd.Series,\n",
    "    labels: List[str],\n",
    "    seed: int = 42,\n",
    ") -> List[BaselineResult]:\n",
    "    X_num = X.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_num = X_num.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_num.values, y_enc.values,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "        stratify=y_enc.values\n",
    "    )\n",
    "\n",
    "    lr = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=3000, n_jobs=-1))\n",
    "    ])\n",
    "    lr.fit(X_train, y_train)\n",
    "    pred_lr = lr.predict(X_test)\n",
    "    rep_lr = classification_report(y_test, pred_lr, target_names=labels, zero_division=0)\n",
    "    cm_lr = confusion_matrix(y_test, pred_lr, labels=np.arange(len(labels)))\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=300, random_state=seed, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred_rf = rf.predict(X_test)\n",
    "    rep_rf = classification_report(y_test, pred_rf, target_names=labels, zero_division=0)\n",
    "    cm_rf = confusion_matrix(y_test, pred_rf, labels=np.arange(len(labels)))\n",
    "\n",
    "    mlp = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", MLPClassifier(hidden_layer_sizes=(256, 128), activation=\"relu\", max_iter=50, random_state=seed))\n",
    "    ])\n",
    "    mlp.fit(X_train, y_train)\n",
    "    pred_mlp = mlp.predict(X_test)\n",
    "    rep_mlp = classification_report(y_test, pred_mlp, target_names=labels, zero_division=0)\n",
    "    cm_mlp = confusion_matrix(y_test, pred_mlp, labels=np.arange(len(labels)))\n",
    "\n",
    "    return [\n",
    "        BaselineResult(\"LogisticRegression\", rep_lr, cm_lr),\n",
    "        BaselineResult(\"RandomForest\", rep_rf, cm_rf),\n",
    "        BaselineResult(\"MLP\", rep_mlp, cm_mlp),\n",
    "    ]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Graph utilities\n",
    "# =============================\n",
    "def make_stratified_masks_from_y(\n",
    "    y: np.ndarray,\n",
    "    train: float = 0.7,\n",
    "    val: float = 0.15,\n",
    "    seed: int = 42,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    n = len(y)\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "        idx, y, test_size=(1.0 - train), random_state=seed, stratify=y\n",
    "    )\n",
    "    val_frac_of_tmp = val / (1.0 - train)\n",
    "    idx_val, idx_test = train_test_split(\n",
    "        idx_tmp, test_size=(1.0 - val_frac_of_tmp), random_state=seed, stratify=y_tmp\n",
    "    )\n",
    "\n",
    "    train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "    train_mask[idx_train] = True\n",
    "    val_mask[idx_val] = True\n",
    "    test_mask[idx_test] = True\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def add_reverse_edges_hetero(data: HeteroData, rev_prefix: str = \"rev_\") -> HeteroData:\n",
    "    for (src, rel, dst) in list(data.edge_types):\n",
    "        if rel.startswith(rev_prefix):\n",
    "            continue\n",
    "        rev_rel = f\"{rev_prefix}{rel}\"\n",
    "        if (dst, rev_rel, src) in data.edge_types:\n",
    "            continue\n",
    "        ei = data[(src, rel, dst)].edge_index\n",
    "        data[(dst, rev_rel, src)].edge_index = ei.flip(0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def torch_confusion_matrix(y_true: torch.Tensor, y_pred: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.long, device=y_true.device)\n",
    "    for t, p in zip(y_true.view(-1), y_pred.view(-1)):\n",
    "        cm[t.long(), p.long()] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def macro_f1_from_cm_np(cm: np.ndarray) -> float:\n",
    "    cm = cm.astype(float)\n",
    "    tp = np.diag(cm)\n",
    "    precision = tp / np.clip(cm.sum(axis=0), 1, None)\n",
    "    recall = tp / np.clip(cm.sum(axis=1), 1, None)\n",
    "    f1 = 2 * precision * recall / np.clip(precision + recall, 1e-12, None)\n",
    "    return float(np.mean(f1))\n",
    "\n",
    "\n",
    "def standardize_flow_x_inplace(data: HeteroData) -> None:\n",
    "    x = data[\"flow\"].x.detach().cpu().numpy()\n",
    "    train_mask = data[\"flow\"].train_mask.detach().cpu().numpy().astype(bool)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x[train_mask])\n",
    "    x_all = scaler.transform(x)\n",
    "    x_all = np.nan_to_num(x_all, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    data[\"flow\"].x = torch.tensor(x_all, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def compute_class_weights_from_train(data: HeteroData) -> torch.Tensor:\n",
    "    y = data[\"flow\"].y.detach().cpu().numpy()\n",
    "    train_mask = data[\"flow\"].train_mask.detach().cpu().numpy().astype(bool)\n",
    "    y_tr = y[train_mask]\n",
    "    counts = np.bincount(y_tr)\n",
    "    counts = np.clip(counts, 1, None)\n",
    "    w = 1.0 / counts\n",
    "    w = w * (len(counts) / w.sum())\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# HetGNN\n",
    "# =============================\n",
    "@torch.no_grad()\n",
    "def sample_fixed_neighbors(edge_index: torch.Tensor, num_src: int, k: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    src, dst = edge_index[0], edge_index[1]\n",
    "    device = edge_index.device\n",
    "\n",
    "    perm = torch.argsort(src)\n",
    "    src_s, dst_s = src[perm], dst[perm]\n",
    "\n",
    "    uniq, cnt = torch.unique_consecutive(src_s, return_counts=True)\n",
    "    ptr = torch.zeros(num_src + 1, device=device, dtype=torch.long)\n",
    "    ptr[uniq + 1] = cnt\n",
    "    ptr = ptr.cumsum(0)\n",
    "\n",
    "    out = torch.full((num_src, k), -1, device=device, dtype=torch.long)\n",
    "    mask = torch.zeros((num_src, k), device=device, dtype=torch.bool)\n",
    "\n",
    "    deg = ptr[1:] - ptr[:-1]\n",
    "    for j in range(k):\n",
    "        has = deg > j\n",
    "        idx = ptr[:-1] + j\n",
    "        out[has, j] = dst_s[idx[has]]\n",
    "        mask[has, j] = True\n",
    "    return out, mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_flow_neighbor_cache(data: HeteroData, k: int = 1, src_type: str = \"flow\"):\n",
    "    num_src = data[src_type].num_nodes\n",
    "    cache = {}\n",
    "    for et in data.edge_types:\n",
    "        if et[0] != src_type:\n",
    "            continue\n",
    "        neigh, mask = sample_fixed_neighbors(data[et].edge_index, num_src=num_src, k=k)\n",
    "        cache[et] = (neigh, mask)\n",
    "    return cache\n",
    "\n",
    "\n",
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: Optional[int], num_nodes: int, hidden: int, max_id_embed: int = 200_000):\n",
    "        super().__init__()\n",
    "        self.has_x = in_dim is not None\n",
    "        if self.has_x:\n",
    "            self.lin = nn.Linear(in_dim, hidden)\n",
    "            self.emb = None\n",
    "            self.emb_size = None\n",
    "        else:\n",
    "            self.emb_size = int(min(num_nodes, max_id_embed))\n",
    "            self.emb = nn.Embedding(self.emb_size, hidden)\n",
    "            self.lin = None\n",
    "\n",
    "    def forward(self, x_or_ids: torch.Tensor) -> torch.Tensor:\n",
    "        if self.has_x:\n",
    "            return self.lin(x_or_ids)\n",
    "        ids = x_or_ids % self.emb_size\n",
    "        return self.emb(ids)\n",
    "\n",
    "\n",
    "class HetGNN(nn.Module):\n",
    "    def __init__(self, data: HeteroData, hidden: int = 128, out: Optional[int] = None, k: int = 1, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.metadata = data.metadata()\n",
    "        node_types, edge_types = self.metadata\n",
    "        if out is None:\n",
    "            out = int(data[\"flow\"].y.max().item() + 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.enc = nn.ModuleDict()\n",
    "        for nt in node_types:\n",
    "            in_dim = data[nt].x.size(-1) if (\"x\" in data[nt]) else None\n",
    "            self.enc[nt] = NodeEncoder(in_dim, data[nt].num_nodes, hidden)\n",
    "\n",
    "        self.groups = [et for et in edge_types if et[0] == \"flow\"]\n",
    "        self.cache = build_flow_neighbor_cache(data, k=k, src_type=\"flow\")\n",
    "\n",
    "        self.gru = nn.ModuleDict()\n",
    "        self.nei_att = nn.ModuleDict()\n",
    "        for et in self.groups:\n",
    "            name = self._gname(et)\n",
    "            self.gru[name] = nn.GRU(hidden, hidden // 2, batch_first=True, bidirectional=True)\n",
    "            self.nei_att[name] = nn.Linear(hidden, 1)\n",
    "\n",
    "        self.mix_att = nn.Linear(2 * hidden, 1)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out),\n",
    "        )\n",
    "\n",
    "    def _gname(self, et) -> str:\n",
    "        return f\"{et[0]}__{et[1]}__{et[2]}\"\n",
    "\n",
    "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
    "        device = data[\"flow\"].y.device\n",
    "        N = data[\"flow\"].num_nodes\n",
    "\n",
    "        if \"x\" in data[\"flow\"]:\n",
    "            f_self = self.enc[\"flow\"](data[\"flow\"].x)\n",
    "        else:\n",
    "            ids = torch.arange(N, device=device)\n",
    "            f_self = self.enc[\"flow\"](ids)\n",
    "\n",
    "        f_self = F.dropout(f_self, p=self.dropout, training=self.training)\n",
    "        F_list = [f_self]\n",
    "\n",
    "        for et in self.groups:\n",
    "            name = self._gname(et)\n",
    "            dst_type = et[2]\n",
    "\n",
    "            neigh_ids, neigh_mask = self.cache[et]\n",
    "            neigh_ids = neigh_ids.to(device)\n",
    "            neigh_mask = neigh_mask.to(device)\n",
    "            safe_ids = neigh_ids.clamp(min=0)\n",
    "\n",
    "            if \"x\" in data[dst_type]:\n",
    "                x = data[dst_type].x[safe_ids]\n",
    "                h0 = self.enc[dst_type](x)\n",
    "            else:\n",
    "                h0 = self.enc[dst_type](safe_ids)\n",
    "\n",
    "            h0 = h0 * neigh_mask.unsqueeze(-1)\n",
    "            gru_out, _ = self.gru[name](h0)\n",
    "\n",
    "            scores = self.nei_att[name](gru_out).squeeze(-1)\n",
    "            scores = scores.masked_fill(~neigh_mask, -1e9)\n",
    "            alpha = F.softmax(scores, dim=1)\n",
    "            alpha = alpha * neigh_mask.float()\n",
    "            alpha = alpha / alpha.sum(dim=1, keepdim=True).clamp(min=1e-12)\n",
    "\n",
    "            f_nei = (alpha.unsqueeze(-1) * gru_out).sum(dim=1)\n",
    "            f_nei = F.dropout(f_nei, p=self.dropout, training=self.training)\n",
    "            F_list.append(f_nei)\n",
    "\n",
    "        F_stack = torch.stack(F_list, dim=1)\n",
    "        f_self_rep = f_self.unsqueeze(1).expand(-1, F_stack.size(1), -1)\n",
    "        mix_in = torch.cat([F_stack, f_self_rep], dim=-1)\n",
    "        mix_scores = self.leakyrelu(self.mix_att(mix_in)).squeeze(-1)\n",
    "        mix_alpha = F.softmax(mix_scores, dim=1)\n",
    "        E = (mix_alpha.unsqueeze(-1) * F_stack).sum(dim=1)\n",
    "\n",
    "        return self.classifier(E)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# HGT / R-GCN\n",
    "# =============================\n",
    "class HGTFlowClassifier(nn.Module):\n",
    "    def __init__(self, data: HeteroData, hidden=64, out=None, layers=2, heads=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        if not _HGT_AVAILABLE:\n",
    "            raise RuntimeError(\"HGTConv is not available in this environment.\")\n",
    "        assert hidden % heads == 0, \"hidden must be divisible by heads\"\n",
    "\n",
    "        self.metadata = data.metadata()\n",
    "        node_types, _ = self.metadata\n",
    "        if out is None:\n",
    "            out = int(data[\"flow\"].y.max().item() + 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.proj = nn.ModuleDict()\n",
    "        self.emb = nn.ModuleDict()\n",
    "\n",
    "        for n in node_types:\n",
    "            if \"x\" in data[n]:\n",
    "                self.proj[n] = Linear(data[n].x.size(-1), hidden)\n",
    "            else:\n",
    "                self.emb[n] = nn.Embedding(data[n].num_nodes, hidden)\n",
    "\n",
    "        try:\n",
    "            _ = HGTConv(hidden, hidden, metadata=self.metadata, heads=heads)\n",
    "        except TypeError as e:\n",
    "            raise RuntimeError(\"HGTConv failed to initialize (likely torch/pyg mismatch).\") from e\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            HGTConv(hidden, hidden, metadata=self.metadata, heads=heads)\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "        self.cls = Linear(hidden, out)\n",
    "\n",
    "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
    "        x_dict = {}\n",
    "        for n in data.node_types:\n",
    "            if n in self.proj:\n",
    "                x = self.proj[n](data[n].x)\n",
    "            else:\n",
    "                x = self.emb[n].weight\n",
    "            x_dict[n] = x\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, data.edge_index_dict)\n",
    "            x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "            x_dict = {k: F.dropout(v, p=self.dropout, training=self.training) for k, v in x_dict.items()}\n",
    "        return self.cls(x_dict[\"flow\"])\n",
    "\n",
    "\n",
    "class RGCNFlowClassifier(nn.Module):\n",
    "    def __init__(self, data: HeteroData, hidden=64, out=None, layers=2, num_bases=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.metadata = data.metadata()\n",
    "        node_types, edge_types = self.metadata\n",
    "\n",
    "        if out is None:\n",
    "            out = int(data[\"flow\"].y.max().item() + 1)\n",
    "\n",
    "        self.node_types = list(node_types)\n",
    "        self.node_type_to_id = {n: i for i, n in enumerate(self.node_types)}\n",
    "        self.flow_type_id = self.node_type_to_id[\"flow\"]\n",
    "\n",
    "        homo = data.to_homogeneous()\n",
    "        self.register_buffer(\"edge_index\", homo.edge_index)\n",
    "        self.register_buffer(\"edge_type\", homo.edge_type)\n",
    "        self.register_buffer(\"node_type\", homo.node_type)\n",
    "\n",
    "        for n, tid in self.node_type_to_id.items():\n",
    "            idx = (self.node_type == tid).nonzero(as_tuple=False).view(-1)\n",
    "            self.register_buffer(f\"idx__{n}\", idx)\n",
    "        self.register_buffer(\"flow_idx\", (self.node_type == self.flow_type_id).nonzero(as_tuple=False).view(-1))\n",
    "\n",
    "        self.num_nodes = int(homo.num_nodes)\n",
    "        self.num_relations = len(edge_types)\n",
    "        self.hidden = hidden\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.proj = nn.ModuleDict()\n",
    "        self.emb = nn.ModuleDict()\n",
    "        for n in self.node_types:\n",
    "            if \"x\" in data[n]:\n",
    "                self.proj[n] = Linear(data[n].x.size(-1), hidden)\n",
    "            else:\n",
    "                self.emb[n] = nn.Embedding(data[n].num_nodes, hidden)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            RGCNConv(hidden, hidden, num_relations=self.num_relations, num_bases=num_bases)\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "        self.cls = Linear(hidden, out)\n",
    "\n",
    "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
    "        x = torch.empty((self.num_nodes, self.hidden), device=self.edge_index.device)\n",
    "\n",
    "        for n in self.node_types:\n",
    "            idx = getattr(self, f\"idx__{n}\")\n",
    "            if n in self.proj:\n",
    "                x[idx] = self.proj[n](data[n].x)\n",
    "            else:\n",
    "                x[idx] = self.emb[n].weight\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, self.edge_index, self.edge_type)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        logits_all = self.cls(x)\n",
    "        return logits_all[self.flow_idx]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Graph train/eval\n",
    "# =============================\n",
    "def train_epoch_fullbatch(model: nn.Module, data: HeteroData, opt: torch.optim.Optimizer, class_w: Optional[torch.Tensor]) -> float:\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    logits = model(data)\n",
    "    y = data[\"flow\"].y\n",
    "    loss = F.cross_entropy(\n",
    "        logits[data[\"flow\"].train_mask],\n",
    "        y[data[\"flow\"].train_mask],\n",
    "        weight=class_w\n",
    "    )\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return float(loss.item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_fullbatch(model: nn.Module, data: HeteroData, split: str = \"val\"):\n",
    "    model.eval()\n",
    "    logits = model(data)\n",
    "    y = data[\"flow\"].y\n",
    "    mask = data[\"flow\"].val_mask if split == \"val\" else data[\"flow\"].test_mask\n",
    "    pred = logits[mask].argmax(dim=-1)\n",
    "    y_true = y[mask]\n",
    "\n",
    "    num_classes = int(y.max().item() + 1)\n",
    "    cm = torch_confusion_matrix(y_true, pred, num_classes=num_classes).detach().cpu().numpy()\n",
    "\n",
    "    acc = float((pred == y_true).float().mean().item())\n",
    "    f1 = macro_f1_from_cm_np(cm)\n",
    "    return acc, f1, cm, y_true.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def fit_graph(\n",
    "    model: nn.Module,\n",
    "    data: HeteroData,\n",
    "    *,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 2e-3,\n",
    "    wd: float = 1e-4,\n",
    "    standardize_flow: bool = True,\n",
    "    class_weighted_loss: bool = True,\n",
    "    device: Optional[torch.device] = None,\n",
    "    name: str = \"graph\",\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if standardize_flow:\n",
    "        standardize_flow_x_inplace(data)\n",
    "\n",
    "    class_w = None\n",
    "    if class_weighted_loss:\n",
    "        class_w = compute_class_weights_from_train(data)\n",
    "\n",
    "    data = data.to(device)\n",
    "    model = model.to(device)\n",
    "    if class_w is not None:\n",
    "        class_w = class_w.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        loss = train_epoch_fullbatch(model, data, opt, class_w)\n",
    "        val_acc, val_f1, _, _, _ = eval_fullbatch(model, data, split=\"val\")\n",
    "\n",
    "        if val_f1 > best_val:\n",
    "            best_val = val_f1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if ep % max(1, epochs // 5) == 0 or ep == 1:\n",
    "            print(f\"[{name}] ep={ep:03d} loss={loss:.4f} val_acc={val_acc:.4f} val_macroF1={val_f1:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    test_acc, test_f1, test_cm, y_true, y_pred = eval_fullbatch(model, data, split=\"test\")\n",
    "    return test_acc, test_f1, test_cm, y_true, y_pred\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Dataset loaders/builders\n",
    "# =============================\n",
    "def load_iot23_csv(path: str = \"IOT-23.csv\") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, na_values=[\"\", \"NA\", \"NaN\", \"-\"])\n",
    "    comb = \"tunnel_parents   label   detailed-label\"\n",
    "    if comb in df.columns and \"label\" not in df.columns:\n",
    "        df[[\"tunnel_parents\", \"label\", \"detailed_label\"]] = df[comb].astype(str).str.split(r\"\\s+\", n=2, expand=True)\n",
    "        df = df.drop(columns=[comb])\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_iot23_binary(df_iot: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.Series, HeteroData, List[str]]:\n",
    "    df = df_iot.copy()\n",
    "    df = df[df[\"label\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    df[\"id.orig_p\"] = df[\"id.orig_p\"].astype(int)\n",
    "    df[\"id.resp_p\"] = df[\"id.resp_p\"].astype(int)\n",
    "    df[\"proto\"] = df[\"proto\"].astype(str)\n",
    "    df[\"service\"] = df[\"service\"].fillna(\"unknown\").astype(str)\n",
    "    df[\"conn_state\"] = df[\"conn_state\"].astype(str)\n",
    "\n",
    "    df[\"ts_dt\"] = pd.to_datetime(df[\"ts\"], unit=\"s\", errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ts_dt\"]).reset_index(drop=True)\n",
    "    df[\"time_window\"] = df[\"ts_dt\"].dt.floor(\"5min\").astype(str)\n",
    "\n",
    "    label_map = {\"Benign\": 0, \"Malicious\": 1}\n",
    "    y = df[\"label\"].map(label_map).astype(int)\n",
    "    labels = [\"Benign\", \"Malicious\"]\n",
    "\n",
    "    FLOW_FEATS = [\"duration\", \"orig_bytes\", \"resp_bytes\", \"orig_pkts\", \"resp_pkts\", \"missed_bytes\"]\n",
    "    X = sanitize_numeric_frame(df, cols=FLOW_FEATS, transform=\"log1p\", clip_lower=0.0)\n",
    "\n",
    "    df[\"flow_id\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "    def make_id_map(series: pd.Series):\n",
    "        uniq = series.astype(str).unique()\n",
    "        return {v: i for i, v in enumerate(uniq)}\n",
    "\n",
    "    ip_map = make_id_map(pd.concat([df[\"id.orig_h\"].astype(str), df[\"id.resp_h\"].astype(str)], ignore_index=True))\n",
    "    port_map = make_id_map(pd.concat([df[\"id.orig_p\"].astype(str), df[\"id.resp_p\"].astype(str)], ignore_index=True))\n",
    "    proto_map = make_id_map(df[\"proto\"].astype(str))\n",
    "    service_map = make_id_map(df[\"service\"].astype(str))\n",
    "    time_map = make_id_map(df[\"time_window\"].astype(str))\n",
    "\n",
    "    flow_ids = df[\"flow_id\"].values\n",
    "    src_ip = df[\"id.orig_h\"].astype(str).map(ip_map).astype(np.int64).values\n",
    "    dst_ip = df[\"id.resp_h\"].astype(str).map(ip_map).astype(np.int64).values\n",
    "    src_port = df[\"id.orig_p\"].astype(str).map(port_map).astype(np.int64).values\n",
    "    dst_port = df[\"id.resp_p\"].astype(str).map(port_map).astype(np.int64).values\n",
    "    proto = df[\"proto\"].astype(str).map(proto_map).astype(np.int64).values\n",
    "    service = df[\"service\"].astype(str).map(service_map).astype(np.int64).values\n",
    "    timew = df[\"time_window\"].astype(str).map(time_map).astype(np.int64).values\n",
    "\n",
    "    data = HeteroData()\n",
    "    data[\"flow\"].x = torch.tensor(X.values, dtype=torch.float32)\n",
    "    data[\"flow\"].y = torch.tensor(y.values.astype(np.int64), dtype=torch.long)\n",
    "\n",
    "    data[\"ip\"].num_nodes = len(ip_map)\n",
    "    data[\"port\"].num_nodes = len(port_map)\n",
    "    data[\"proto\"].num_nodes = len(proto_map)\n",
    "    data[\"service\"].num_nodes = len(service_map)\n",
    "    data[\"time\"].num_nodes = len(time_map)\n",
    "\n",
    "    data[\"flow\", \"to_ip_orig\", \"ip\"].edge_index = torch.tensor(np.vstack([flow_ids, src_ip]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_ip_resp\", \"ip\"].edge_index = torch.tensor(np.vstack([flow_ids, dst_ip]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_port_orig\", \"port\"].edge_index = torch.tensor(np.vstack([flow_ids, src_port]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_port_resp\", \"port\"].edge_index = torch.tensor(np.vstack([flow_ids, dst_port]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_proto\", \"proto\"].edge_index = torch.tensor(np.vstack([flow_ids, proto]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_service\", \"service\"].edge_index = torch.tensor(np.vstack([flow_ids, service]), dtype=torch.long)\n",
    "    data[\"flow\", \"to_time\", \"time\"].edge_index = torch.tensor(np.vstack([flow_ids, timew]), dtype=torch.long)\n",
    "\n",
    "    data = add_reverse_edges_hetero(data)\n",
    "    tr, va, te = make_stratified_masks_from_y(y.values.astype(int), seed=seed)\n",
    "    data[\"flow\"].train_mask, data[\"flow\"].val_mask, data[\"flow\"].test_mask = tr, va, te\n",
    "    return X, pd.Series(y.values.astype(int)), data, labels\n",
    "\n",
    "\n",
    "def load_cic_csv(path: str = \"CIC.csv\") -> pd.DataFrame:\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "\n",
    "def build_cic_label(df_cic: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.Series, HeteroData, List[str]]:\n",
    "    df = df_cic.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    label_col = \"label\" if \"label\" in df.columns else (\"Label\" if \"Label\" in df.columns else None)\n",
    "    if label_col is None:\n",
    "        raise KeyError(\"CIC: couldn't find label/Label column.\")\n",
    "\n",
    "    required = [\"Source IP\", \"Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\", label_col]\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"CIC missing column: {c}\")\n",
    "\n",
    "    df = df.dropna(subset=required).reset_index(drop=True)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(df[label_col].astype(str).values).astype(np.int64)\n",
    "    labels = list(le.classes_)\n",
    "\n",
    "    entity = {\"Source IP\", \"Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\", label_col}\n",
    "    num_cols = [c for c in df.select_dtypes(include=[\"number\"]).columns if c not in entity]\n",
    "    if len(num_cols) == 0:\n",
    "        raise ValueError(\"CIC: no numeric flow features found after excluding entity columns.\")\n",
    "\n",
    "    X = sanitize_numeric_frame(df, cols=num_cols, transform=\"signed_log1p\", clip_lower=None)\n",
    "\n",
    "    def make_id_map(series: pd.Series):\n",
    "        uniq = series.astype(str).unique()\n",
    "        return {v: i for i, v in enumerate(uniq)}\n",
    "\n",
    "    ip_map = make_id_map(pd.concat([df[\"Source IP\"].astype(str), df[\"Destination IP\"].astype(str)], ignore_index=True))\n",
    "    port_map = make_id_map(pd.concat([df[\"Source Port\"].astype(str), df[\"Destination Port\"].astype(str)], ignore_index=True))\n",
    "    proto_map = make_id_map(df[\"Protocol\"].astype(str))\n",
    "\n",
    "    flow_ids = np.arange(len(df), dtype=np.int64)\n",
    "    src_ip = df[\"Source IP\"].astype(str).map(ip_map).astype(np.int64).values\n",
    "    dst_ip = df[\"Destination IP\"].astype(str).map(ip_map).astype(np.int64).values\n",
    "    src_port = df[\"Source Port\"].astype(str).map(port_map).astype(np.int64).values\n",
    "    dst_port = df[\"Destination Port\"].astype(str).map(port_map).astype(np.int64).values\n",
    "    proto = df[\"Protocol\"].astype(str).map(proto_map).astype(np.int64).values\n",
    "\n",
    "    data = HeteroData()\n",
    "    data[\"flow\"].x = torch.tensor(X.values, dtype=torch.float32)\n",
    "    data[\"flow\"].y = torch.tensor(y_enc, dtype=torch.long)\n",
    "\n",
    "    data[\"ip\"].num_nodes = len(ip_map)\n",
    "    data[\"port\"].num_nodes = len(port_map)\n",
    "    data[\"proto\"].num_nodes = len(proto_map)\n",
    "\n",
    "    data[\"flow\", \"src_ip\", \"ip\"].edge_index = torch.tensor(np.vstack([flow_ids, src_ip]), dtype=torch.long)\n",
    "    data[\"flow\", \"dst_ip\", \"ip\"].edge_index = torch.tensor(np.vstack([flow_ids, dst_ip]), dtype=torch.long)\n",
    "    data[\"flow\", \"src_port\", \"port\"].edge_index = torch.tensor(np.vstack([flow_ids, src_port]), dtype=torch.long)\n",
    "    data[\"flow\", \"dst_port\", \"port\"].edge_index = torch.tensor(np.vstack([flow_ids, dst_port]), dtype=torch.long)\n",
    "    data[\"flow\", \"uses_proto\", \"proto\"].edge_index = torch.tensor(np.vstack([flow_ids, proto]), dtype=torch.long)\n",
    "\n",
    "    data = add_reverse_edges_hetero(data)\n",
    "    tr, va, te = make_stratified_masks_from_y(y_enc, seed=seed)\n",
    "    data[\"flow\"].train_mask, data[\"flow\"].val_mask, data[\"flow\"].test_mask = tr, va, te\n",
    "    return X, pd.Series(y_enc), data, labels\n",
    "\n",
    "\n",
    "def load_unsw_csv(path: str = \"unswnb15.csv\") -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def build_unsw_attackcat(df_unsw: pd.DataFrame, min_samples: int = 1000, seed: int = 42) -> Tuple[pd.DataFrame, pd.Series, HeteroData, List[str]]:\n",
    "    df = df_unsw.copy()\n",
    "\n",
    "    # Baseline preprocessing aligned with your snippet\n",
    "    df_mc = df.copy()\n",
    "    drop_cols = [\"proto\", \"service\", \"state\", \"label\"]\n",
    "    drop_cols = [c for c in drop_cols if c in df_mc.columns]\n",
    "    df_mc = df_mc.drop(columns=drop_cols)\n",
    "\n",
    "    y_raw = df_mc[\"attack_cat\"].astype(str)\n",
    "    X_tab = df_mc.drop(columns=[\"attack_cat\"])\n",
    "\n",
    "    valid = y_raw.value_counts()\n",
    "    valid = valid[valid >= min_samples].index\n",
    "    mask = y_raw.isin(valid)\n",
    "\n",
    "    X_tab = X_tab.loc[mask].reset_index(drop=True)\n",
    "    y_raw = y_raw.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y_raw.values).astype(np.int64)\n",
    "    labels = list(le.classes_)\n",
    "\n",
    "    # Graph build (flow -> proto/service/state)\n",
    "    df_g = df.loc[mask].reset_index(drop=True)\n",
    "    df_g[\"flow_id\"] = np.arange(len(df_g), dtype=np.int64)\n",
    "\n",
    "    proto_vals = df_g[\"proto\"].astype(str).unique()\n",
    "    service_vals = df_g[\"service\"].astype(str).unique()\n",
    "    state_vals = df_g[\"state\"].astype(str).unique()\n",
    "    proto_map = {v: i for i, v in enumerate(proto_vals)}\n",
    "    service_map = {v: i for i, v in enumerate(service_vals)}\n",
    "    state_map = {v: i for i, v in enumerate(state_vals)}\n",
    "\n",
    "    flow_ids = df_g[\"flow_id\"].values\n",
    "    proto = df_g[\"proto\"].astype(str).map(proto_map).astype(np.int64).values\n",
    "    service = df_g[\"service\"].astype(str).map(service_map).astype(np.int64).values\n",
    "    state = df_g[\"state\"].astype(str).map(state_map).astype(np.int64).values\n",
    "\n",
    "    preferred = [\"dur\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sload\", \"dload\", \"tcprtt\", \"synack\", \"ackdat\"]\n",
    "    if all(c in df_g.columns for c in preferred):\n",
    "        feat_cols = preferred\n",
    "    else:\n",
    "        exclude = {\"label\", \"attack_cat\", \"proto\", \"service\", \"state\", \"flow_id\"}\n",
    "        feat_cols = [c for c in df_g.select_dtypes(include=[\"number\"]).columns if c not in exclude]\n",
    "        if len(feat_cols) == 0:\n",
    "            raise ValueError(\"UNSW: no numeric features found for flow.x.\")\n",
    "\n",
    "    X_flow = sanitize_numeric_frame(df_g, cols=feat_cols, transform=\"signed_log1p\", clip_lower=None)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data[\"flow\"].x = torch.tensor(X_flow.values, dtype=torch.float32)\n",
    "    data[\"flow\"].y = torch.tensor(y_enc, dtype=torch.long)\n",
    "\n",
    "    data[\"proto\"].num_nodes = len(proto_map)\n",
    "    data[\"service\"].num_nodes = len(service_map)\n",
    "    data[\"state\"].num_nodes = len(state_map)\n",
    "\n",
    "    data[\"flow\", \"uses_proto\", \"proto\"].edge_index = torch.tensor(np.vstack([flow_ids, proto]), dtype=torch.long)\n",
    "    data[\"flow\", \"uses_service\", \"service\"].edge_index = torch.tensor(np.vstack([flow_ids, service]), dtype=torch.long)\n",
    "    data[\"flow\", \"has_state\", \"state\"].edge_index = torch.tensor(np.vstack([flow_ids, state]), dtype=torch.long)\n",
    "\n",
    "    data = add_reverse_edges_hetero(data)\n",
    "    tr, va, te = make_stratified_masks_from_y(y_enc, seed=seed)\n",
    "    data[\"flow\"].train_mask, data[\"flow\"].val_mask, data[\"flow\"].test_mask = tr, va, te\n",
    "    return X_tab, pd.Series(y_enc), data, labels\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Runner\n",
    "# =============================\n",
    "def run_one_dataset(\n",
    "    ds_name: str,\n",
    "    X: pd.DataFrame,\n",
    "    y_enc: pd.Series,\n",
    "    data: HeteroData,\n",
    "    class_labels: List[str],\n",
    "    out_reports: str,\n",
    "    out_cms: str,\n",
    "    epochs: int = 50,\n",
    "):\n",
    "    print(f\"\\n===== {ds_name} =====\")\n",
    "    vc = pd.Series(y_enc.values).value_counts().sort_index()\n",
    "    print(\"Class distribution:\", vc.to_dict())\n",
    "\n",
    "    # quick sanity on flow.x\n",
    "    print(debug_tensor_stats(f\"{ds_name}/flow.x\", data[\"flow\"].x))\n",
    "\n",
    "    # ---- Baselines ----\n",
    "    results = run_baselines(X, y_enc, labels=class_labels, seed=42)\n",
    "    for r in results:\n",
    "        save_text(os.path.join(out_reports, f\"{ds_name}__{r.name}__report.txt\"), r.report)\n",
    "        save_confusion_matrix(r.cm, class_labels,\n",
    "                              os.path.join(out_cms, f\"{ds_name}__{r.name}__cm_raw\"),\n",
    "                              f\"{ds_name} - {r.name}\",\n",
    "                              normalize=None)\n",
    "        save_confusion_matrix(r.cm, class_labels,\n",
    "                              os.path.join(out_cms, f\"{ds_name}__{r.name}__cm_norm\"),\n",
    "                              f\"{ds_name} - {r.name}\",\n",
    "                              normalize=\"true\")\n",
    "\n",
    "    # ---- Graph models ----\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _save_graph(model_name: str, y_true: np.ndarray, y_pred: np.ndarray, cm: np.ndarray):\n",
    "        rep = classification_report(y_true, y_pred, target_names=class_labels, zero_division=0)\n",
    "        save_text(os.path.join(out_reports, f\"{ds_name}__{model_name}__report.txt\"), rep)\n",
    "        save_confusion_matrix(cm, class_labels,\n",
    "                              os.path.join(out_cms, f\"{ds_name}__{model_name}__cm_raw\"),\n",
    "                              f\"{ds_name} - {model_name}\",\n",
    "                              normalize=None)\n",
    "        save_confusion_matrix(cm, class_labels,\n",
    "                              os.path.join(out_cms, f\"{ds_name}__{model_name}__cm_norm\"),\n",
    "                              f\"{ds_name} - {model_name}\",\n",
    "                              normalize=\"true\")\n",
    "\n",
    "    # HetGNN\n",
    "    het = HetGNN(data, hidden=128, k=1, dropout=0.2)\n",
    "    acc, f1, cm, y_true, y_pred = fit_graph(\n",
    "        het, data, epochs=epochs, device=device, name=f\"{ds_name} HetGNN\",\n",
    "        standardize_flow=True, class_weighted_loss=True\n",
    "    )\n",
    "    print(f\"{ds_name} HetGNN test_acc={acc:.4f} test_macroF1={f1:.4f}\")\n",
    "    _save_graph(\"HetGNN\", y_true, y_pred, cm)\n",
    "\n",
    "    # R-GCN\n",
    "    rgcn = RGCNFlowClassifier(data, hidden=64, layers=2, num_bases=8, dropout=0.2)\n",
    "    acc, f1, cm, y_true, y_pred = fit_graph(\n",
    "        rgcn, data, epochs=epochs, device=device, name=f\"{ds_name} R-GCN\",\n",
    "        standardize_flow=True, class_weighted_loss=True\n",
    "    )\n",
    "    print(f\"{ds_name} R-GCN test_acc={acc:.4f} test_macroF1={f1:.4f}\")\n",
    "    _save_graph(\"RGCN\", y_true, y_pred, cm)\n",
    "\n",
    "    # HGT (optional)\n",
    "    if _HGT_AVAILABLE:\n",
    "        try:\n",
    "            hgt = HGTFlowClassifier(data, hidden=64, layers=2, heads=2, dropout=0.2)\n",
    "            acc, f1, cm, y_true, y_pred = fit_graph(\n",
    "                hgt, data, epochs=epochs, device=device, name=f\"{ds_name} HGT\",\n",
    "                standardize_flow=True, class_weighted_loss=True\n",
    "            )\n",
    "            print(f\"{ds_name} HGT test_acc={acc:.4f} test_macroF1={f1:.4f}\")\n",
    "            _save_graph(\"HGT\", y_true, y_pred, cm)\n",
    "        except Exception as e:\n",
    "            warn = f\"HGT skipped for {ds_name} due to: {repr(e)}\"\n",
    "            print(warn)\n",
    "            save_text(os.path.join(out_reports, f\"{ds_name}__HGT__SKIPPED.txt\"), warn)\n",
    "    else:\n",
    "        warn = f\"HGTConv not available -> HGT skipped for {ds_name}.\"\n",
    "        print(warn)\n",
    "        save_text(os.path.join(out_reports, f\"{ds_name}__HGT__SKIPPED.txt\"), warn)\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "\n",
    "    OUT = \"outputs\"\n",
    "    out_reports = os.path.join(OUT, \"reports\")\n",
    "    out_cms = os.path.join(OUT, \"confusion_matrices\")\n",
    "    ensure_dir(out_reports)\n",
    "    ensure_dir(out_cms)\n",
    "\n",
    "    # ---- Load local CSVs ----\n",
    "    df_iot = load_iot23_csv(\"/kaggle/input/cybersecurity-dataset/IOT-23.csv\")\n",
    "    df_cic = load_cic_csv(\"/kaggle/input/cybersecurity-dataset/CIC.csv\")\n",
    "    df_unsw = load_unsw_csv(\"/kaggle/input/cybersecurity-dataset/unswnb15.csv\")\n",
    "\n",
    "    # ---- IoT-23 (binary) ----\n",
    "    X_iot, y_iot, data_iot, labels_iot = build_iot23_binary(df_iot, seed=42)\n",
    "    run_one_dataset(\"IoT23_binary\", X_iot, y_iot, data_iot, labels_iot, out_reports, out_cms, epochs=100)\n",
    "\n",
    "    # ---- CIC (label) ----\n",
    "    X_cic, y_cic, data_cic, labels_cic = build_cic_label(df_cic, seed=42)\n",
    "    run_one_dataset(\"CIC_label\", X_cic, y_cic, data_cic, labels_cic, out_reports, out_cms, epochs=100)\n",
    "\n",
    "    # ---- UNSW (attack_cat multi-class) ----\n",
    "    X_unsw, y_unsw, data_unsw, labels_unsw = build_unsw_attackcat(df_unsw, min_samples=1000, seed=42)\n",
    "    run_one_dataset(\"UNSW_attack_cat\", X_unsw, y_unsw, data_unsw, labels_unsw, out_reports, out_cms, epochs=100)\n",
    "\n",
    "    print(\"\\nDone. Check outputs/\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T16:44:58.846747Z",
     "iopub.status.busy": "2026-01-29T16:44:58.846422Z",
     "iopub.status.idle": "2026-01-29T16:44:59.049478Z",
     "shell.execute_reply": "2026-01-29T16:44:59.048789Z",
     "shell.execute_reply.started": "2026-01-29T16:44:58.846716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/outputs/reports/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__MLP__report.txt (deflated 68%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__LogisticRegression__report.txt (deflated 62%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__LogisticRegression__report.txt (deflated 55%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__HGT__report.txt (deflated 67%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__LogisticRegression__report.txt (deflated 68%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__HGT__report.txt (deflated 61%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__RandomForest__report.txt (deflated 56%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__HetGNN__report.txt (deflated 61%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__RGCN__report.txt (deflated 60%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__HetGNN__report.txt (deflated 62%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__MLP__report.txt (deflated 62%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__RandomForest__report.txt (deflated 68%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__RGCN__report.txt (deflated 62%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__HetGNN__report.txt (deflated 67%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__HGT__report.txt (deflated 62%)\n",
      "  adding: kaggle/working/outputs/reports/CIC_label__RGCN__report.txt (deflated 67%)\n",
      "  adding: kaggle/working/outputs/reports/IoT23_binary__MLP__report.txt (deflated 55%)\n",
      "  adding: kaggle/working/outputs/reports/UNSW_attack_cat__RandomForest__report.txt (deflated 63%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/reports.zip /kaggle/working/outputs/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T16:45:00.422329Z",
     "iopub.status.busy": "2026-01-29T16:45:00.421552Z",
     "iopub.status.idle": "2026-01-29T16:45:00.819328Z",
     "shell.execute_reply": "2026-01-29T16:45:00.818644Z",
     "shell.execute_reply.started": "2026-01-29T16:45:00.422286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/outputs/confusion_matrices/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__MLP__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HGT__cm_norm.png (deflated 9%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RandomForest__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RandomForest__cm_raw.png (deflated 23%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__MLP__cm_norm.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RGCN__cm_norm.png (deflated 24%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RandomForest__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RGCN__cm_raw.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RandomForest__cm_raw.pdf (deflated 30%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HGT__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__MLP__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RGCN__cm_norm.pdf (deflated 33%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__LogisticRegression__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RandomForest__cm_norm.png (deflated 28%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__MLP__cm_norm.png (deflated 29%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HetGNN__cm_raw.png (deflated 28%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RGCN__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HGT__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HGT__cm_raw.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HetGNN__cm_norm.pdf (deflated 33%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RGCN__cm_raw.png (deflated 24%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HGT__cm_raw.png (deflated 29%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HGT__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__LogisticRegression__cm_raw.png (deflated 25%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__MLP__cm_norm.png (deflated 25%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RandomForest__cm_raw.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__MLP__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RandomForest__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RandomForest__cm_norm.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HGT__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__MLP__cm_raw.png (deflated 28%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HetGNN__cm_norm.png (deflated 24%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HetGNN__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RandomForest__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HetGNN__cm_norm.png (deflated 29%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__LogisticRegression__cm_raw.png (deflated 22%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__MLP__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__LogisticRegression__cm_norm.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RandomForest__cm_norm.png (deflated 23%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RGCN__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RandomForest__cm_raw.png (deflated 27%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HetGNN__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__LogisticRegression__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__LogisticRegression__cm_norm.png (deflated 23%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RGCN__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__LogisticRegression__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HGT__cm_norm.png (deflated 29%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HGT__cm_norm.png (deflated 25%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RGCN__cm_raw.png (deflated 29%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RGCN__cm_norm.png (deflated 28%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__RGCN__cm_norm.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__LogisticRegression__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RGCN__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HetGNN__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__LogisticRegression__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HetGNN__cm_raw.png (deflated 24%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__RandomForest__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HetGNN__cm_raw.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__MLP__cm_raw.pdf (deflated 30%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__LogisticRegression__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HetGNN__cm_norm.png (deflated 9%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HetGNN__cm_norm.pdf (deflated 32%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HGT__cm_raw.pdf (deflated 31%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__HGT__cm_norm.pdf (deflated 33%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__LogisticRegression__cm_raw.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__MLP__cm_raw.png (deflated 24%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__RGCN__cm_norm.png (deflated 9%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__LogisticRegression__cm_norm.png (deflated 27%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__HetGNN__cm_raw.png (deflated 10%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/CIC_label__MLP__cm_norm.pdf (deflated 33%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/IoT23_binary__HGT__cm_raw.png (deflated 25%)\n",
      "  adding: kaggle/working/outputs/confusion_matrices/UNSW_attack_cat__MLP__cm_raw.png (deflated 10%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/cm.zip /kaggle/working/outputs/confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9366745,
     "sourceId": 14662094,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
